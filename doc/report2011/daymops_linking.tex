%% nabbed from http://tex.stackexchange.com/questions/19982/how-do-i-add-parfor-in-algorithmic-environment
% declaration of the new block
\algblock{ParFor}{EndParFor}
% customising the new block
\algnewcommand\algorithmicparfor{\textbf{parfor}}
\algnewcommand\algorithmicpardo{\textbf{do}}
\algnewcommand\algorithmicendparfor{\textbf{end\ parfor}}
\algrenewtext{ParFor}[1]{\algorithmicparfor\ #1\ \algorithmicpardo}
\algrenewtext{EndParFor}{\algorithmicendparfor}


\section{The Linking Stages of DayMOPS}
\label{linking}

Sufficiently bright moving objects which are observed by the LSST
telescope will generate diaSource detections, stored in the LSST
diaSource detection catalog.  The LSST diaSource detection catalog
will also hold detections from a variety of non-moving object sources,
including transient phenomena and artifacts of image processing.  One
of the major responsibilities of dayMOPS is to link these diaSources
into tracklets and tracks, as described earlier (see
Section~\ref{daymopsOverview}). 


\subsection{Linear and Quadratic and more: Models of Motion}

\begin{figure}[ht]
  \centering
    \includegraphics[width=10cm]{illustrations/4.png}
    \caption[Object motion across the sky.]{A plot of simulated observations of object 40185 on the
      sky.  Though the observations span only one week, a pronounced
      quadratic path is visible.  Note that the object is observed two
      or more times on each night.}
\label{objectMotion}
\end{figure}


In order to discover and identify new objects, astronomers have
traditionally used sky-plane approximations to predict and model the
behavior of solar system objects for which a true orbit is not yet
known.  As a general rule of thumb, objects are said to move linearly
(with a more or less fixed velocity) in RA and Dec over the course of
a single night and quadratically (having velocity and some
acceleration) in RA and Dec over the course of a month.  These are, of
course, approximations, and linear and quadratic fits will inevitably
contain some error.  An example of one object with a clear quadratic
path over seven days is show in Figure~\ref{objectMotion}.

In general, these approximations become closer to the actual sky-plane
motion of the object as objects are observed at larger distances from
the Earth or at solar elongations closer to opposition. It's worth
noting however, that for faster moving objects such as NEOs or objects
observed further from opposition, these approximations start to break
down; for example, NEOs do show acceleration over the course of a
night (although generally not significant acceleration over 90
minutes) and any moving object near turn-around (where its motion
appears to reverse direction on the sky) will provide a poor fit to
quadratic motion over a month.

These basic approximations are our starting point and are used to determine
which detections could plausibly be linked. For tracklets, this is
simple; diaSources which can be linked with linear motion become
tracklets. For tracks, however, first we determine which diaSources
could potentially be joined into tracks (using the velocity
information from the tracklets along the way) using quadratic motion,
but then this track undergoes some further scrutiny before being
marked as `valid'. 

When we are determining the validity of tracks, by looking at
residuals between the observed positions and the quadratic
approximation of motion (and rejecting detections or whole tracks
which do not have low enough residuals), we have found that allowing
for residuals high enough to account for errors due to the quadratic
approximation itself permits too many mislinkages. The discovery rate of
false tracks is hundreds of times the rate of discovering true tracks;
we become swamped with false detections. 

To avoid this, when determining the validity of a track, we now
require stricter tests.  The primary goal here is to reduce the
residuals to allow for tighter filtering of the potential tracks. The
first step is to attempt to add a topocentric correction to the
detections; during the night, as the Earth rotates, objects which are
close to Earth will have some additional `wiggle' in their motion due
to the change in observer's location.  The topocentric correction fits
the range (1/distance) of the object from the tracklet, and applies a
correction to the RA locations of the tracklets that alter the
observed (topocentric) values to equivalent geocentric values. The Dec
values are left unchanged, as this primarily effects the RA. The
second step is to attempt to fit a higher-order polynomial to the
resulting RA and Dec values. If a higher-order than quadratic
polynomial can be supported for the track, ({\it i.e.} if
the fit residuals are lower than all of the errors between the
predicted positions and the measured positions, the order
of the fit is too high), then this higher
order fit is done. After the fit for the track is
created (either quadratic or higher order, potentially with a
topocentric correction, but always separately fit in RA/Dec), then a
chi-squared probability is computed for the track. We reject tracks
with chi-squared values larger than a predetermined cutoff; this
cutoff is determined experimentally to optimize the true / false track
ratio. 

Note that tracklets and tracks represent hypothetical linkages, many
of which may be incorrect.  The linking algorithms are greedy, and
intended to permit finding as many moving objects as possible. Thus, a
single detection may exist in several tracklets and/or several tracks
and a given tracklet may be found in multiple tracks, shown in
Figure~\ref{objectLinking}.  However, once a diaSource is linked into
an actual Orbit, it will not be joined into any further tracklets or
tracks or other moving objects. 


\begin{figure}[ht]
  \centering
    \includegraphics[width=16cm]{illustrations/oneObjectMops.png}
    \caption[Example dayMOPS linkages.]{An illustration of how DayMOPS linkage might be applied
      to a single object.  In $i$, the object is observed on three
      nights; on the first and last night, it gets two detections per
      night, but on the second night it gets four.  In $ii$, initial
      tracklets are generated; time separation of visits on the second
      night is such that we get two tracklets.  In $iii$, we merge the
      tracklets from the second night so there are only three
      tracklets.  In $iv$, we attempt inter-nightly linking and
      generate a single track.}
\label{objectLinking}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%
%%% findTracklets
%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Building Tracklets : findTracklets}

\textbf{Tracklets} are linkages between DiaSource detections occuring
within the same night. By creating tracklets, DayMOPS can find
sky-plane position and velocity estimates for sets of detections which
may belong to the same solar system objects.  The use of tracklets
also simplifies the downstream work of track generation, which
attempts to find sets of detections with a good
position/velocity/acceleration fit on the sky-plane; since tracklets
have known position and velocity, the track generation phase needs
only to find those tracklets compatible within some acceleration
factor.

Correctly-linked tracklets from a given object are needed to generate
a good track for that object and eventually discover its orbit.
However, if these useful tracklets are too deeply buried among very
large numbers of other tracklets, then the job of tracklet linking
will become extremely slow and expensive.  Generally, these other,
unwanted tracklets are false tracklets (mislinkages between detections
not attributable to the same object), though in special conditions
large numbers of correctly-linked but redundant tracklets can cause
pain as well (this will discussed in \ref{collapseTracklets}).

In order to ensure that tracklet-generating images are acquired, it is
necessary to ensure that fields of the sky are visited two or more
times within an accepted time period each night. To constrain the
number of tracklets, we impose a maximum apparent velocity on the
tracklets, and also require that sky fields be revisited within a
fairly short time period ($\leq 90$ minutes is the current rule).
Raising the maximum velocity threshold enables one to find
faster-moving objects, and raising the maximum allowed revisit time
also enables one to generate tracklets in more fields of the sky;
however, increasing either of these thresholds also increases the
search space and can significantly increase the number of mislinked
tracklets, greatly increasing the cost downstream.

The process of initial tracklet creation is accomplished by the
findTracklets software.  Later refinement of tracklets is accomplished
by collapseTracklets and additional filters, primarily purifyTracklets. 

\subsubsection{Algorithm} 

The findTracklets software is responsible for finding pairs of
detections which occur within a fixed time threshold, and have
apparent velocity below a given threshold.  For a given detection and
a set of image times, one can calculate the maximum distance an object
could have travelled at each time using the velocity limit.  To find
detections with which the query detection could be linked, one can
imagine searching a circular region in the later images based on this
distance.

\begin{figure}[ht]
  \centering
    \includegraphics[width=6cm]{illustrations/findTracklets-onequery.png}
    \caption[An example of findTracklets endpoints.]{An example of searching for compatible second endpoints
      for a given detection.  The first detection and each of the
      second endpoints will be used to create a new tracklet.}
\label{findTrackletsIllustrated}
\end{figure}


This can be accomplished in a fairly straightforward way through the use of
KD-Trees.  KD-Trees are hierarchical data structures which allow for quickly and
efficiently performing range searches on points in space
\citep{bentley_kdtrees}.  We use KD-Trees for many different purposes
in MOPS, but what these uses have in common is exploiting the
hierarchical nature of the trees; using the fact that by checking the
boundaries at a high-level on a branch we may rule out having to
compare any of the data stored at lower levels on that
branch. 

A KD-Tree-based method for building
tracklets was first contributed by Jeremy Kubica for his PhD thesis
\citep{kubica_thesis}.  For findTracklets, 2-Dimensional KD-Trees are
used, covering the space of (RA, Dec).  Given a detection and trees
containing detections from later images, we can use range searches to
quickly find nearby detections in those later images and use them for
the creation of tracklets.


\begin{figure}[ht]
\hrulefill
\begin{algorithmic}[5]
\Require $I$ is a set of images, each of which has an associated exposure time and contains a set of detections
\State \Comment{Create a 2D KD-Tree for each image, holding detections from that image.}
\State $T \gets \emptyset$
\For {$i \in I$}
  \State $t \gets$ Make2DTree$(i.detections)$
  \State $t.time \gets i.time$
  \State $T \gets T \cup \{t\}$
\EndFor
\State \Comment{Use these trees to discover the actual tracklets.}
\State $tracklets \gets \emptyset$
\For {$t_1 \in T$}
  \State $later \gets \{t_i \in T : 0 < t_i.time - t_1.time < maxDt\}$
  \For{$d \in t_1.detections$}
     \For{$t_q \in later$}
 
       \State \Comment{Use time between images and max velocity to
         calculate the max travel distance}

        \State $dt \gets t_q.time - t_1.time$
        \State $dd \gets dt * maxV$
        \State \Comment{Use KD-Tree range search to find detections within max travel distance}
        \State $tracklets \gets tracklets \cup t_q.$rangeSearch($d.ra, d.dec, dd$)
     \EndFor
   \EndFor
\EndFor
\Return{$tracklets$}
\end{algorithmic}
\hrulefill
\caption[findTracklets psuedocode.]{Pseudo-code for the findTracklets algorithm.  2D (RA, Dec)
  trees are created for each image; for each detection, later trees
  are searched for nearby detections. }
 \label{findTrackletsAlgorithm}
\end{figure}


Because the sky is a sphere, notions of `distance' and `velocity' must
be handled carefully, especially near the poles.  Both the KD-Tree
library used and the findTracklets software use actual great-circle
distance and velocity for their queries, avoiding problems near the
poles.  The software should also be impervious to wrap-around errors -
objects which move between, say, $359.9 \degree$ in RA and $.01
\degree$ in RA will be detected.  The Appendix \ref{kdTreeLib}
explains the KD-Tree library used in greater detail.

The findTracklets software itself only finds pairs of diaSources which
could potentially be moving objects; if particular solar system object
was observed four times in one night, it could potentially generate
six separate tracklets, depending on the time of the observations and
the time limits imposed by findTracklets. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5
%% COLLAPSE TRACKLETS & purify tracklets
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%5


\subsection{Merging Tracklets} \label{collapseTracklets}

Finding multiple tracklets for a single object makes later stages of
linking inefficient; if every object was observed four times per
night, the number of tracklets could be increased by a factor of six -
at both the start and endpoints of the tracks - resulting in a 36 times
increase in the number of output tracks. Observing fields of the sky
(and thus, generating multiple tracklets) more than twice in a night
is common with our current OpSim runs, and is actually desireable for
several purposes. In addition, the `Deep Drilling' fields observe fields many
more than twice per night. In fields observed $n$ times per night, the
number of tracklets can grow like $O(n^2)$, potentially generating
huge numbers of tracklets. Merging tracklets these tracklet pairs, as
appropriate, removes the inefficiencies for later stages of linking,
thus there are some post-findTracklets merging and filtering stages to
address this issue. 

The `collapseTracklets' software 
attempts to join colinear 2-detection tracklets into longer (3
detections or more) tracklets, while the `purifyTracklets' software
attempts to avoid the risk of merging mislinked with true tracklets.

In addition, it is sometimes possible that a tracklet may link
together a set of detections already present in another higher
cardinality (longer) tracklet; the `removeSubsets' software finds and
removes these shorter subsets. The subset removal algorithm can be
used for tracks as well as tracklets, and so is presented in
section~\ref{subsetRemoval} after the description of linkTracklets.


\subsubsection{CollapseTracklets} 

In collapseTracklets, a method similar to the Hough transform is used
to identify roughly colinear tracks and merge them. 
An intermediate time, $t_c$ is selected (we use the average time of
the first and last detections) and use the apparent linear motion of
the tracklets to project their location at $t_c$.  We then store these
projected (RA,Dec) locations and the angle/velocity of each tracklet.
At this point, colinear tracklets should have similar positions and
motion vectors, making them easy to find.  This is accomplished with a
series of range searches, which of course can be implemented with 4-D
(RA, Dec, angle, velocity) KD-Trees.  The full pseudo-code is
presented in Figure \ref{collapseTrackletsAlgorithm}.

More information on collapseTracklets is available in Jon Myer's
Master's thesis, titled `Methods for Solar System Object Searching in
Deep Stacks', submitted 2008 to the Dept of Computer Science at U of
Arizona. A PDF of this document is available in the LSST git
repository of dayMOPS/docs. 

Code and usage:  The collapseTracklets algorithm is implemented in {\tt
  collapseTracklets.h} and {\tt collapseTracklets.cc}.  A command-line
interface is implemented in {\tt collapseTrackletsMain.cc}.  Run {\tt
  collapseTracklets -h} for usage hints.

\begin{figure}[ht]
\hrulefill
\begin{algorithmic}[5]
  \Require $T$ is a set of intra-nightly tracklets, $D$ is the set of nightly detections from which $T$ was created, $range$ is a 4-tuple of tolerances for RA, Dec, angle and velocity.
  \State $t_c \gets midpoint(\{ d_{time} : d \in D \})$
  \For {$t \in T$}
    \State Calculate $t$'s predicted location at time $t_c$, its motion angle and velocity
  \EndFor
  \State \Comment{Create a 4D KD-Tree of the tracklets on their projected RA, Dec position and motion angle/velocity.}
  \State $tree \gets$ Make4DTree$(T)$
  \State $outTracklets = \emptyset$
  \For {$t \in T,\ t$ has not already merged with another tracklet}
    \State \Comment{Find tracklets with projected location, motion similar to that of $t$}
    \State $candidates \gets tree.$rangeSearch$(t_{projected\ position}, t_{angle}, t_{velocity}, range)$
    \For {$c \in candidates$} 
      \If{$c$ and $t$ do not contain different detections from the same image}
        \State $t.detections \gets t.detections \cup c.detections$
        \State mark $c$ as already merged
      \EndIf
    \EndFor
    \State mark $t$ as already merged
    \State $outTracklets \gets outTracklets \cup t$
  \EndFor
  \Return{$outTracklets$}
\end{algorithmic}
\hrulefill
\caption[collapseTracklets psuedocode.]{Pseudo-code for the collapseTracklets algorithm. A 4-D KD-Tree over RA, Dec, angle, velocity is constructed using the projected locations and motion of the tracklets.  Tracklets which are similar in this 4-D space are roughly colinear, so they are merged and written to output.}
\label{collapseTrackletsAlgorithm} 
\end{figure}

Issue: Currently, collapseTracklets handles wrap-around, but otherwise treats
the sky as a flat (RA, Dec) plane when calculating the projected
positions of tracklets.  This is acceptable for tracklets close to the
ecliptic, but not sufficient closer to the poles.  This should be
fixed when possible.

Issue: Choosing an appropriate set of thresholds for collapseTracklets may be
difficult; with variable time between images, the amount of error on
velocity may differ from tracklet to tracklet.  Other factors come
into play as well; higher thresholds will lead to more correct
linkages as well as more incorrect linkages - which, as described in
the following section, can be ``undone'' later by purifyTracklets.  We
arrived at our current thresholds through simple trial and
error. These threshholds will have to be evaluated for use with real
data with real astrometric errors and time variability. 

\subsubsection{PurifyTracklets}
PurifyTracklets examines the merged tracklets and removes detections
if they are sufficiently far from the best-fit line. The algorithm is
presented in Figure~\ref{purifyTrackletsAlgorithm}. 

\begin{figure}[ht]
\hrulefill
\begin{algorithmic}[5]
  \Require $T$ is a set of tracklets, $rmsMax$ is a maximum root-mean
  squared residual on the tracklet's best-fit function to its
  detections

  \For {$t \in T$}
    \State $rms = RMS(t)$
    \While {$rms > maxRms, |t| > 2$} 
      \State remove the worst-fitting detection from $t$
      \State $rms = RMS(t)$
    \EndWhile
  \EndFor
  \Return{$outTracklets$}
\end{algorithmic}
\hrulefill
\caption[purifyTracklets pseudocode.]{In purifyTracklets, poorly-fitted detections are ``pruned''
  from tracklets. In certain degenerate cases, we may prune tracklets
  down to only two detections, in which case the two-detection
  tracklet is kept.}
\label{purifyTrackletsAlgorithm}
\end{figure}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%          LINKTRACKLETS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Building Tracks: linkTracklets}

With tracklets already assembled, we should have many linkages
representing the position, location, and velocity of the various
objects observed, as well as ``false'' mislinked tracklets.  While
tracklets have only linear motion, tracks have more complex paths; in the
track building phase, this is always calculated as a pair of quadratic
functions in RA and Dec, rather than a single motion vector.  By
making use of this quadratic approximation of motion, valid for
approximately one month, we can move a sliding window of up to 30 days
over the data, looking for tracklets which could be linked by some
quadratic acceleration factor.  As we find these linkages, depending
on the detections themselves (the number and timespan of detections, the best-fit
distance to each object (based on the tracklet velocity), their
astrometric errors) we will also apply a topocentric correction to the
detections and a higher-order fit in order to apply a stronger
requirement on the residuals to the fit when outputting tracks. 

Generally, the track generation phase is the most computationally
resource intensive task in DayMOPS processing. Unlike other stages,
which consider only a night's-worth of data at a time, it must
consider tracklets from many nights and find linkages between them. In
order to build a useful track, suitable for orbit fitting, we need to
link tracklets from three separate nights, within our sliding window
of time (generally we use 15 days for this time window, to make the
computations manageable). 
In general, this task scales exponentially with the number of
tracklets, the time between observations, and the velocity limits for the
tracklets. Thus it can quickly become overwhelming with dense source
data, high velocity and acceleration limits, poor merging of
tracklets, and long time windows.  

To tackle this problem, we use an algorithm for performing this track
discovery presented by Kubica et al. (\citet{kubica_thesis},
\citet{Kubica:2005:MTA:1081870.1081889}).  In essence, the idea behind
the algorithm is to build per-image 4D-Trees of (RA position, Dec
position, RA velocity, Dec velocity), and use these to hold the
tracklets, indexed by position/velocity.  We then imagine the KD-Trees
as hierarchical bounding boxes, and consider pairs of bounding boxes
from different nights, calculating whether they could be linked by
some acceleration factor less than our maximum acceleration threshold.
If the boxes could be linked, then a track may exist within their
contents and we continue searching bounding boxes lower in the tree
hierarchy; if not, we know that no track of interest to us could pass
through the boxes and we can abandon searching immediately.  By using
the hierarchical structure of the KD-Trees, we can avoid searching in
large areas of tracklet-space where no track could ever exist, greatly
reducing our workload. While these are similar to the KD-trees used in
findTracklets, they are more complex because of the higher
dimensionality (4D instead of 2D) and there are many more trees
involved (each individual image is
represented by a tree instead of one tree for an entire night's data,
as in findTracklets). 

\subsubsection{Recursive Tree-walk Using Pruning}
\label{searchPruning}

In the linkTracklets algorithm, all tracklets starting in a given
image are placed in a single 4D-Tree containing bounding boxes in (RA
position, Dec position, RA velocity, Dec velocity)-space.  One tree is
created for each of the images. It is then possible to calculate the
acceleration needed for an object in one bounding box in one tree to
reach ({\it i.e.} be compatible with) the second bounding box in a
later tree.  Because we are generally interested in tracks which have
acceleration within a fixed range (e.g. between $>.02 deg/day^2$ and
$<-.02 deg/day^2$), we can abandon searching at a given pair of
bounding boxes if the necessary acceleration is outside our range of
interest.

The minimum and maximum acceleration connecting two bounding boxes is
currently calculated as follows:

\begin{equation}
maxAcc = \min  \left(\begin{array}{ccc} & \displaystyle \frac{Node2.maxV - Node1.minV}{dt} \\
& \displaystyle \frac{2}{dt^2} \bigg(Node2.maxP_0 - Node1.minP_0 - Node1.minV \times dt \bigg) \\
& \displaystyle \frac{2}{dt^2} \bigg(Node1.maxP_0 - Node2.minP_0 + Node2.maxV \times dt \bigg) \end{array}\right)
% & \displaystyle parentMaxAcc, \\ jeremy says this shouldn't happen, even though it's in the code
\label{maxAcc}
\end{equation}

\begin{equation}
minAcc  = \max  \left(\begin{array}{ccc} & \displaystyle \frac{Node2.minV - Node1.maxV}{dt},\\
& \displaystyle \frac{2}{dt^2} \bigg( Node2.minP_0 - Node1.maxP_0 - \displaystyle Node1.maxV \times dt\bigg), \\
& \displaystyle \frac{2}{dt^2} \bigg(Node1.minP_0 - Node2.maxP_0 + Node2.minV \times dt\bigg) \end{array} \right)
%   & parentMinAcc, jeremy says this shouldn't happen, even though its in the code.
\label{minAcc}
\end{equation}


Issue: Note that this approach simplifies the problem by treating the sky as
a flat plane, which will be problematic near the poles.  However, the
above calculation appears to be the ``hot spot'' of the linkTracklets
algorithm and accounts for most of the computation time, and so
simplifying to reduce floating point costs greatly improves
performance.

In the code, this calculation is performed by the function 
{\tt updateAccBoundsReturnValidity} in {\tt linkTracklets.cc}.

Pruning allows the rapid avoidance of searching in areas where no track
can exist. As a simplified introduction to the
full algorithm, see Figure~\ref{simplifiedLinkTracklets} for a
two-tracklet-linking, ``endpoint-only'' version of the algorithm which
finds pairs of compatible tracklets on different nights.

\begin{figure}[ht!]
\hrulefill
\begin{algorithmic}[5]
  \Require{$nodeA$ and $nodeB$ are KD-Tree nodes which hold tracklets
    from two different images on different nights, $minAcc$ and $maxAcc$
    specify the limits of accelerations which the user finds
    interesting.}
  
  \State $accRange = $ min/max acceleration to move from $nodeA$ to $nodeB$
  \If{$accRange$ does not overlap $(minAcc, maxAcc)$}
  \Return $\emptyset$
  \Else
  \If{$nodeA$ and $nodeB$ are leaf nodes}
  
  \State \Comment{When we hit a pair of terminal nodes, and their
    acceleration bounds are interesting, then we have found a set
      of tracklets which may be sufficient to create a track.}
    
    \Return $\{$tryToBuildATrack($nodeA$.tracklet, $nodeB$.tracklet)$\}$
    \Else
    
    \State \Comment{In order to ensure that this function sees
      nodes which are roughly the same size, we choose the larger
      node and ``split'' that one, recursing on its children.}


    \State $largerNode, smallerNode \gets orderBySize(nodeA, nodeB)$
    \State $leftRes \gets recurse(largerNode.\text{leftChild}, smallerNode, S)$
    \State $rightRes \gets recurse(largerNode.\text{rightChild}, smallerNode, S)$
    \Return $ leftRes \cup rightRes $
    \EndIf
    \EndIf    
  \end{algorithmic}
\hrulefill
  \caption[Simplified linkTracklets pseudocode.]{An endpoint-matching, two-tracklet version of the
    linkTracklets algorithm.  Note that if two nodes have no chance at
    holding a compatible tracklet (first ``if'' check) then their
    children are never searched; only if they may hold an interesting
    track are the children searched.  In this way, the algorithm
    avoids even examining a great number of KD-Tree node pairs and
    thus the pairs tracklets contained therein.}
  \label{simplifiedLinkTracklets}
\end{figure}

\paragraph{Support Tracklets, Support Nodes, and the Full LinkTracklets Algorithm}

For orbit determination, we require detections on at least three separate
nights.  The endpoint-only algorithm in \ref{simplifiedLinkTracklets}
will only attempt to find pairs of tracklets, giving tracks with only
two nights of observational data.  In practice, we seek to find tracks
with tracklets from three unique nights.  This could be accomplished
using various extensions to the endpoint-only algorithm, but it is
argued in \citet{kubica_thesis} and
\citet{Kubica:2005:MTA:1081870.1081889} that by far the most efficient
of these variants is called the algorithm called the {\bf vtrees} (for
``variable trees'') algorithm.

In the vtrees algorithm, we search for tracks with one or more
intermediate ``support'' tracklets in between the ``endpoint'' or
``model'' tracklets (the first and last tracklet).  To accomplish
this, the vtrees algorithm searches for compatible endpoint nodes as
in Figure~\ref{simplifiedLinkTracklets} but, as search progresses, maintains
a list of compatible ``support'' nodes - nodes which could hold useful
intermediate tracklets between the tracklets in the endpoint nodes.
These are filtered at each step, again using the equations
\ref{maxAcc} and \ref{minAcc}.  As the search descends through the
possible valid combinations of endpoint nodes, the support list is
filtered and refined.  When search terminates at a pair of leaf nodes,
the support nodes are used to find possible support tracklets.  If the
support list ever becomes empty, we can prune the searching at this
point, since we know no useful track (no track with at least three
nights of data) could exist between the endpoint
nodes.

The full vtrees algorithm is presented in
Figure~\ref{linkTrackletsAlgorithm}.  This is the actual algorithm
implemented by the function {\tt doLinkingRecurse} from {\tt linkTracklets.cc}


\begin{figure}[ht!]
\hrulefill
\begin{algorithmic}[5]
\Require{$nodeA$ and $nodeB$ are KD-Tree nodes which hold tracklets
  from two different images on different nights, $S$ holds a series of
  nodes from images take on nights in between $nodeA.time$ and
  $nodeB.time$, $minAcc$ and $maxAcc$ specify the limits of
  accelerations which the user finds interesting.}

\State $accRange = $ min/max acceleration to move from $nodeA$ to $nodeB$
\If{$accRange$ does not overlap $(minAcc, maxAcc)$}
    \Return $\emptyset$
  \Else
  
  \For{$supportNode \in S$}
  \If{$supportNode$ represents an awkwardly large portion of tracklet-space relative to $nodeA$ and $nodeB$}
    \State replace $supportNode$ with $supportNode.rightChild$ and $supportNode.leftChild$, coming back to them later
    \Else
    \If{no track with acceleration within $accRange$ could pass from $nodeA$ through $supportNode$ and into $nodeB$}
      \State remove $supportNode$ from $S$
    \EndIf
  \EndIf

  \If{$S$ is empty}
    \Return $\emptyset$ \Comment{There is no way to build a three-tracklet track using the contents of these nodes, so abandon searching.}
  \EndIf

  \EndFor
    \If{$nodeA$ and $nodeB$ are leaf nodes}

    \State \Comment{Again, try to build a track, this time using $S$ for intermediate tracklets}

       \Return $\{$tryToBuildATrack($nodeA$.tracklet, $nodeB$.tracklet, $S$)$\}$
    \Else

       \State $largerNode, smallerNode \gets orderBySize(nodeA, nodeB)$
       \State $leftRes \gets recurse(largerNode.\text{leftChild}, smallerNode, S)$
       \State $rightRes \gets recurse(largerNode.\text{rightChild}, smallerNode, S)$
       \Return $ leftRes \cup rightRes $
  \EndIf
\EndIf

\end{algorithmic}
\hrulefill
\caption[linkTracklets pseudocode.]{The full vtrees algorithm, the actual algorithm implemented
  in linkTracklets.  At each recursion, $S$, the set of support nodes,
  is split based on the position, velocity, and acceleration range
  implied by the $nodeA$ and $nodeB$, the two endpoint nodes.  Note
  that if support nodes are periodically ``split'' and replaced with
  their chidlren if they too large relative to $nodeA$ and $nodeB$.}
 \label{linkTrackletsAlgorithm}
\end{figure}

In the algorithm, support nodes are filtered and possibly split (and
their children filtered) at each recursive step.  Choosing when to
split the support nodes is an important performance question.  If we
split too aggressively, then we will add more items to $S$ and be
required to filter a larger number of nodes at each recursion, which
will increase cost at each step.  If we split support nodes too
rarely, then the support nodes may become very large relative to the
endpoint nodes and we will often find that we have some compatible
support node, and thus continue searching - even in cases where, had
we split the support nodes, we would have seen that none of the leaf
nodes held by the larger box were consistent with our
position/velocity/acceleration range.  This leads to needless
searching when we should have simply terminated.

Currently, the cutoff for splitting is based on the spatial size of
the support node relative to that of the two endpoint nodes.  First
a weighting factor, $\alpha$, is calculated:

\begin{equation}
\alpha = \frac{supportNode.time - nodeA.time}{supportNode.time - nodeB.time}
\end{equation}

We then split the node if, for \textit{any} of the spatial axes (RA position, Dec position, RA velocity, or Dec velocity):

\begin{equation}
\bigg(\frac{width(nodeA.axis)}{\alpha} + \alpha \x width(nodeB.axis)\bigg) < 4 \x width(supportNode.axis)
\end{equation}

Where $width$ is the spatial extent of the node.  This approach was
developed by Jeremy Kubica through empirical testing, and may or may
not be optimal for our data; however, in our experience, it seems to
be effective.

The splitting and filtering of the support trees is implemented by
{\tt splitSupportRecursively} in {\tt linkTracklets.cc}.  This
function uses several helper functions, including the same {\tt
  updateAccBoundsReturnValidity} function used to check the
compatibility of endpoint nodes.


\subsubsection{Track Validation}

Once a pair of endpoint tracklets and one or more support tracklets
have been found via the vtrees algorithm, we may attempt to finally
build a track from them.  This is relatively straightforward; three or
more tracklets with compatible acceleration, from three or more
nights, have already been found.  However, this acceleration factor is
rather approximate, as it is based on the position and velocity
estimate of the tracklets, not the best-fit to the detections held in
the tracklet.  Thus, at this point we actually start examining the
detections themselves.

The track fitting is conducted in the method {\tt
  calculateBestFitQuadratic} in {\tt Track.cc}. First the Dec values
of the observations are fit to either a quadratic or higher order
(despite the name of the method) function, using the residuals of the
fit compared to the differences between predicted and actual positions to
control the order of the fit. Then the RA values of the observations
are fit, with the added complication of including a topocentric
correction if one could be determined from the tracklet and is
appropriate for the fitting function. The order of the function in Dec
and RA do not have to be the same.  

In the event there are multiple possible support detections at one of
these image times, the best-fitting detection is chosen. 

After the fit is determined, the chi-squared probability of the RA and
Dec fits is calculated for the `chi-squared probability filter'.  If
an insufficient number of valid detections are found or the
chi-squared probability of the RA or Dec fits is higher than the
predetermined cutoff, the potential track is rejected.  If it is
validated, then it is passed to the set of output tracks.

\subsubsection{Issues and Quirks}
\label{someLinkingIssues}

\paragraph{Topocentric Correction}
We can generate more precise fits and do more aggressive filtering if we first apply a
topocentric correction to the detections, converting from topocentric
coordinates to geocentric coordinates based on an assumed topocentric
distance.  For this reason, before the positions/velocities of the
tracklets are calculated, the trees are built or the linking algorithm
is run, topocentric correction factor is calculated for each
tracklet. This factor is then applied to the RA value of each detection in the
tracklet when the track is being evaluated for residuals between the
best-fit path across the sky (in RA/Dec) vs the locations of the
actual detections.  The correction is only applied in RA rather than
Declination; the effect is small in declination. 

Issue: I would like to see this verified across the entire sky, as it
was developed near the equator/ecliptic. 


\paragraph{Spherical Geometry Issues}
Currently, the tracklet tree constructor in {\tt TrackletTree.cc} will
alter RA (and if needed, Dec) values so that all RA (and possibly Dec)
values are stored on a contiguous 180-degree region.  This simplifies
the critical ``hot-spot'' calculations in {\tt
  updateAccBoundsReturnValidity}, since we do not need to check for
values which cross over the 0/360 line in RA.  However, this means
that if the input data covers too large an area of the sky then the
acceleration range calculation will likely fail, and thus the whole
algorithm will misbehave.

Further, as mentioned in section \ref{searchPruning}, we simplify the
math throughout track generation by treating the sky as a flat plane,
which will lead to significant distortion near the poles.  This
problem is often addressed by rotating the data in question so that it
is centered on the RA, Dec origin (0,0), where polar distortions are
minimal.  This could be done for the entire input set of detections,
but if the detections cover a sufficiently large area of the sky then
we will not significantly reduce the problem.  Temporarily
re-centering the relevant points at each call to {\tt
  updateAccBoundsReturnValidity} would likely work, but would probably
be quite expensive.  This is a problem which needs to be addressed
eventually, perhaps by changing the equations used in
\ref{searchPruning}, or per by rotating the bounding boxes per-query,
so that the endpoint images will always be roughly centered over the
origin.

Issue: Due to manpower constraints, we have not been able to test how big a problem this is, if
the acceleration calculations are failing, and to what degree assuming
the sky is `flat' is distorting acceleration and compatibility
calculations.  It might be interesting to look at placing these trees
into a x/y/z/vx/vy/vz sphere instead of the current RA/Dec/vRA/vDec
coordinates. It is worth remembering that most of our tests have been
using 15 day windows, so over most of the sky we will not have
detections that cover much more than 180 degrees in RA except
potentially near the poles. 


\paragraph{Special Considerations of KD-Tree Construction for linkTracklets} 

Astrometric error on detections will affect both the position and
velocities of true tracklets.  As a result, bounding boxes must be
extended to encompass not just the tracklets which they hold, but the
surrounding error bars.  Each tracklet will have a unique error bar on
its velocity, as tracklets will span different distances in RA and Dec
($dp$) and may also span different numbers of images and thus have
different $dt$:

\begin{eqnarray}
minV = (dp - 2\times astrom\_err)/dt  \\
maxV = (dp + 2\times astrom\_err /dt
\end{eqnarray}

Thus, each node in the tracklet KD-Tree will have its position range extended by:

\begin{eqnarray}
minP_0 = P_0 - astrom\_err \\
maxP_0 = P_0 + astrom\_err  
\end{eqnarray}
\\ and its velocity range will be extended to encompass the $minV$ and
$maxV$ of all its child tracklets.  These rules apply to leaf nodes as
well as non-leaf nodes, which must have bounds at least as great as
their child nodes.  Note that as a result of expanding the boxes
around tracklets, KD-Tree nodes may actually overlap.

Note that for linkTracklets, a leaf node's bounds should only extend
as far as the error bars around their tracklets, and the non-leaf
nodes should encompass only the area around their children's bounds.
One of our early implementations did not follow this rule, and instead
partitioned the space in a ``top-down'' fashion as in the left side of
Figure~\ref{trackletTree}, and it then handled the issue of error bars
in the acceleration calculation function.  This was technically
correct, but lead to horrific performance - 1,000 or 10,000 times
slower than the current version, illustrated on the right side of
Figure~\ref{trackletTree}.  This is because the earlier version had
nodes which encompassed larger areas of tracklet-space and thus sets
of nodes were more likely to appear compatible, which results in less
search pruning.

\begin{figure}[ht!]
\centering
\includegraphics[width=10cm]{illustrations/kdTree_trackletTree.png}
\caption[KD-tree construction example.]{Two possible ways of
  constructing a KD-Tree over four points. The left shows a top-down
  tree; the right shows a tree constructed from the bottom up and with
non-leaf branches extending only as far as their children's bounds. }
\label{trackletTree}
\end{figure}

The former type of tree is simpler to construct (and perhaps easier to
visualize when debugging).  This is the type of tree used in most the
other algorithms, which perform range searches and thus can extend
handle their error bars per-query by extending their ranges as needed.
However, the latter type of tree is the one needed for linkTracklets,
and is implemented by the KD-Tree subclass {\tt TrackletTree} in {\tt
  TrackletTree.cc}.


\paragraph{Performance Enhancements for Acceleration Range Calculation}
As noted earlier, the function {\tt updateAccBoundsReturnValidity},
which implements the equations \ref{maxAcc} and \ref{minAcc},
accounts for most of the CPU time spent in linkTracklets.  Thus,
optimizing performance in this section of code is critical.

Rather than evaluate all arguments to $\min$ and $\max$ functions at
the outset, the code attempts to evaluate each possible argument. If
it becomes clear at any point that $minAcc > maxAcc$, then
we know that the search can be pruned and the function returns
immediately.

As an additional optimization, we start with $minAcc$ and $maxAcc$ set
to the values used by our caller in the recursive searching. We know
that the caller was examining two endpoint nodes which were either
equal to or a parent of our current nodes, and thus our parents
$(minAcc, maxAcc)$ range will be greater than the one we
calculate. (For the initial start of the recursion, we use the
user-specified min/max acceleration thresholds, because we do not care
about any values outside this range anyway.)  By doing this, we
actually have a $maxAcc$ value available as soon as we calculate our
first possible $minAcc$ value and vice-versa; this allows the earliest
possible termination.  However, it does make the code somewhat more
confusing to read.

This confusion is somewhat amplified by the fact that this same {\tt
  updateAccBoundsReturnValidity} function is used for filtering
support nodes.  In this case, the initial $minAcc$ and $maxAcc$ values
are taken from the acceleration range calculated when examining the
endpoint nodes $nodeA$ and $nodeB$, since we are not interested in
finding acceleration values which connect $nodeA$ and a support node
unless they also connect the support node to $nodeB$.

This is not an issue, just an explanation of the reasoning for
including $minAcc$ and $maxAcc$ values from the parent and endpoint
nodes. 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%      TRACK FILTERING
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Subset Removal}
\label{subsetRemoval}

Some tracks are {\bf subset tracks} of other tracks; that is,
occasionally detections linked by one track found by linkTracklets
will be a subset of those linked by another track in the same output
set.  This can arise for a variety of reasons, but occurs most
commonly when a real object generates tracklets on four or more nights
within a linking period. We generally expect that these subset tracks
are unhelpful, and because they increase the size of the set of tracks
sent to orbit determination, they are possibly costly.

Finding and removing subset tracks could be accomplished with a very
naive double-for loop over the set of tracks, but of course this does
not scale to larger data sets ($O(n^2)$ for $n$ tracks).  A more
efficient algorithm uses a detection-to-track ``reverse-map'' $R$,
which maps from each detection to the set of tracks holding that
detection.  This is easy to construct for a set of tracks $T$:

\hrulefill
\begin{algorithmic}[5]
  \For{$t_i \in T$}
  \State $R[d_j] = \{\}$
  \EndFor
  \For{$d_j \in t_i$}
  \State $R[d_j] = R[d_j] \cup t_i$
  \EndFor
\end{algorithmic}
\hrulefill

We may then use the algorithm from
Figure~\ref{subsetRemovalAlgorithm}, which makes use of this
reverse-map.  The underlying idea is this: for each track $t_i$, we
seek to find any track containing all the detections in $t_i$; any
track containing all these detections must be equal to or a superset
of $t_i$.

\begin{figure}[h!]
\hrulefill
\begin{algorithmic}[5]
\For{$t_i \in T$}
  \State candidates = $T$
  \For{$d_j \in t_i$}
    \State candidates = candidates $\cap$ $R[d_j]$
  \EndFor
  \If{$|$candidates$| >$ 1}
    \State $t_i$ is a subset of some other $t_j \in T$; discard it
  \Else 
    \State keep $t_i$
  \EndIf
\EndFor
\end{algorithmic}
\hrulefill
\caption[subsetRemoval pseudocode.]{Psuedocode for the subset removal algorithm}
\label{subsetRemovalAlgorithm}
\end{figure}

Subset tracklets can also occur when collapseTracklets is used.  The
same algorithm and software can be used to remove subset tracklets
from a set of tracklets as well.

The reverse-map is implemented with a C++ {\tt std::map}, allowing
logarithmic-time lookups, and its contents are C++ {\tt std::set}s,
which allowing linear-time intersection calculations.  However, both
structures are implemented with trees; between the tracks themselves
and these tree structures, this algorithm can require significant
amounts of memory, and no distributed-memory equivalent is currently
known to us.  Fortunately, we have had good luck with distributed
shared-memory approaches for large data sets (this is documented
somewhere by Jon Myers in work with SDSC Dash; however, in practice
with the improved filters on the linkTracklets output the large memory
requirement is reduced and we could just run orbit fitting on
everything). 


\subsection{Notes on Software Development}

\subsubsection{Accomodations for Large Data Sets}
\label{largeData}
Over the course of our experiments, we discovered that under some
circumstances, tools may return some very large data sets - larger
than the memory available on our development machines.  Though RAM
sizes may grow over time, it is likely that dayMOPS users will
continue to experiment with increasingly dense noise or loose limits,
resulting in increasingly large numbers of tracklets or tracks.

To help deal with this problem, the {\tt findTracklets} and {\tt
linkTracklets} functions can be configured to output their results in
various ways; they can be configured either to store their results in
memory and return them (much like a normal function call) or to return
nothing and write results directly to file.  If the user is confident
that the data set to be returned will fit in memory, the former is
more elegant (and fits better with the LSST model of passing data to
and from worker nodes through memory only), but for our experiments we
always write to file, in case the number of tracklets or tracks
discovered is large. 

The {\tt findTracklets} and {\tt linkTracklets} functions each take as
an argument an object of type {\tt findTrackletsConfig} or
{\tt linkTrackletsConfig}; each type has a public member variable
called {\tt outputMethod} which can be set.  {\tt findTracklets.h} and
{\tt linkTracklets.h} each contain enum types which can be used to set
these flags.

Dealing with larger-than-memory data sets as input to our software
tools is a more significant problem.  We generally assume that the
number of input detections will fit in memory, and that KD-Trees of
these detections will also fit in memory.  This has always been the
case, and fortunately it is easy to predict whether a set of
detections will fit in memory or not.  However, the number of
tracklets or tracks may, depending on the data and configuration of
the software, grow to be quite large, and is not trivially
predictable.  For software which uses tracklets or tracks as its input
data and operates on them in bulk (including {\tt collapseTracklets},
{\tt removeSubsets}, and {\tt linkTracklets}), this may be problematic.


\subsubsection{Parallelization}
\label{parallelization}

We have parallelized the various linking stages of DayMOPS using
multithreading, implemented using OpenMP (see the code versions ending
with OMP in the repository).  This allows multiple CPU
cores to work simultaneously on the data set, but does not address the
problem of partitioning the data sets between machines.  This means
that multithreading can be effective in large-memory environments, but
does not attempt to solve the problem of larger-than-memory data sets.

The issue of larger-than-memory data sets could be addressed in
several ways: through (OS-level or implementation-level) distributed
shared memory, algorithmic changes, or simply requiring large-memory
machines.  We have explored using kernel-level distributed shared
memory provided by the vSMP software on the Gordon cluster at San
Diego Supercomputing Center.  This software runs inside the OS kernels
of various machines connected via network, and provides the appearance
that all CPUs and RAM on the various machines are shared on a single
motherboard.  A similar effect could be achieved through explicit use
of a user-level distributed shared memory library (such as memcached),
but would require additional coding.

\subsubsubsection{Parallel FindTracklets} In our current version, we
parallelize the work being done per-detection.  This is achieved with
a simple \textbf{parallel for} loop replacing the \textbf{for} loop at
line 12 in the pseudocode at Figure~\ref{findTrackletsAlgorithm}.  Because the amount of work
done will vary per-detection, depending on how many possible second
endpoints are found, we use dynamic thread scheduling as opposed to
static scheduling.  The only critical section used is the writing of
results, otherwise there is no need for inter-processor
synchronization or communication.

The tracklets reported by parallel findTracklets should be identical
to those reported by serial findTracklets, though the order in which
they are reported may differ.


\subsubsubsection{Parallel CollapseTracklets} The work done is
parallelized on a per-tracklet basis.  Again, this is achieved with a
simple \textbf{parallel for} loop replacing the \textbf{for} loop at
line 8 of Figure~\ref{collapseTrackletsAlgorithm}.  The writing of output is
inside a critical section, as in parallel findTracklets.

Unlike findTracklets, the work done in the parallel region of
collapseTracklets is not entirely independent; different threads may
read and write the ``already merged'' flags on tracklets.  This leads
to a potential consistency problem, with several possible solutions.
One approach would be to enforce strict locking on every read/write to
the ``already merged'' flags.  This has the disadvantage of
potentially scaling very badly, since the reads/writes are extremely
frequent and synchronization costs could be quite high.  It is also
problematic in that it creates potential for deadlocks if not
implemented very carefully.  A second approach would be to disregard
the flags entirely, which would be quite simple to implement. However,
in cases where objects may generate many tracklets (e.g. deep stacks,
which can see $n^2$ tracklets given $n$ observations) this would
result in a significant amount of redundant work being performed.  We
took a third approach, which attempts to compromise between these two: we do
not enforce strict locking on the ``already merged'' flags, and simply
allow that there may be stale reads occasionally.  These stale reads
may lead to redundant work, but because we expect that they will be
uncommon in general (perhaps not in deep stacks), this should not have
too big an impact on performance. The redundant work will lead to redundant tracklets in output, but we
expect them to be removed by the removeSubsets stage.

It is also important to note that because of the ``already merged''
flags, results from collapseTracklets are nondeterministic in a
multi-threaded environment.  Consider the unusual case in which
tracklets $t_a$ and $t_b$ are sufficiently close in the tree, and
$t_b$ and $t_c$ are sufficiently close in the tree, but $t_a$ and
$t_c$ are not sufficiently close in the tree, and querying for one
will not return the other.  Depending on which tracklet is first
visited and queried for similar tracklets, tracklet $t_b$ may be
merged with either $t_a$ or $t_c$, at which point it will be flagged
and never considered again.  As a result, output from parallel
collapseTracklets runs is expected to vary, and not just with regard
to ordering, albeit quite slightly.


\subsubsubsection{Parallel LinkTracklets} 

LinkTracklets looks for tracks which could start in a given image and
end in another given image.  Usually, the number of possible
start/endpoint image pairs is fairly large, so this provides a natural
axis of paralellism.  The psuedocode for the parallel linkTracklets is presented in Fig. \ref{parallelLinkTracklets}.


\begin{figure}[h!]
\hrulefill
\begin{algorithmic}[5]
\Require{$T$ is a set of per-image tracklet trees}
\State $work \gets \emptyset$
\For{$t_i \in T$}
  \For{$t_j \in T$ and $t_j$ happens $\ge 2$ nights later than $t_i$}
    \If{there are sufficient support trees between $t_i$ and $t_j$}
    \State add $t_i, t_j$ to work
    \EndIf
  \EndFor
\EndFor

\ParFor{$w \in$ work}
  \State $t_i, t_j \gets w$
  \State $sup \gets $ trees from images between $t_i$ and $t_j$
  \State use vtrees algorithm (Fig. \ref{linkTrackletsAlgorithm}) to find tracks starting in $t_i$, ending in $t_j$ and passing through $sup$ 
\EndParFor
\end{algorithmic}
\hrulefill
\caption[Parallel linkTracklets pseudocode.]{Psuedocode for parallel linkTracklets.  First, usable pairs of images are identified in a single thread, then the searching between these pairs of images is performed in parallel by multiple threads.}
\label{parallelLinkTracklets}
\end{figure}

Unlike the other parallel programs, which write their output inside of
a critical section, parallel linkTracklets maintains separate output
sets for each thread, so it has no critical sections at all.  This
allows for slightly better scaling of performance.  The parallel
linkTracklets should return the same tracks discovered by the
sequential version, though the order of their discovery may differ.


\subsubsubsection{Parallel Subset Removal} 

Again, the subset removal algorithm contains an outer \textbf{for}
loop which provides an obvious axis of parallelism.  The \textbf{for}
loop at line 8 of Figure~\ref{subsetRemovalAlgorithm} is simply
changed to a parallel \textbf{for} loop to achieve multi-threading
parallelism.  

The work done in the parallel section is natrually independent, so the
only critical section is the writing of output.  The results of
parallel subset removal should be identical to that of the sequential
version, except for the ordering.


